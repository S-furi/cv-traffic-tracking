{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34k4KExYLjEP"
      },
      "source": [
        "# YOLO for Object Tracking\n",
        "## Tracking Traffic Signs and Traffic Lights\n",
        "The main goal of this project is to create a model capable of **detecting** and also **tracking** traffic signs and traffic lights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCpgcwtOMTX6"
      },
      "source": [
        "### Dependency\n",
        "Before starting with this notebook, It is important to install the correct dependencies for this project. We will start by downloading the roboflow library for installing our datasets and we will also install ultralytics for importing and use YOLO.\n",
        "\n",
        "### YOLO\n",
        "The YOLO network is highly efficient for object detection tasks. However, we will also need to incorporate other computer vision algorithms to adapt object detection for object tracking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xY6apg4-XLKO"
      },
      "outputs": [],
      "source": [
        "!pip install roboflow\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset\n",
        "First and foremost, it is essential to download the dataset that will be used for training the YOLOv11 architecture. Additionally, we will download a series of videos from another dataset to check and test whether our architecture functions correctly."
      ],
      "metadata": {
        "id": "0qpX5p2w0cLC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArW7ozddS3Id"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "from roboflow import Roboflow\n",
        "\n",
        "traffic_lights_video_path = kagglehub.dataset_download('matteoiorio/traffic-lights-video')\n",
        "\n",
        "rf = Roboflow(api_key=\"Ex8Yj8EiaKSeCDoNeUms\")\n",
        "project = rf.workspace(\"ithb-5ka4m\").project(\"lisa-traffic-light-detection-8vuch\")\n",
        "version = project.version(3)\n",
        "traffic_lights = version.download(\"yolov11\")\n",
        "\n",
        "project = rf.workspace(\"usmanchaudhry622-gmail-com\").project(\"traffic-and-road-signs\")\n",
        "version = project.version(1)\n",
        "traffic_signs = version.download(\"yolov11\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PfXlcNPcWvzN"
      },
      "outputs": [],
      "source": [
        "print(traffic_lights.location)\n",
        "print(traffic_signs.location)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoKuO0X9Mnfz"
      },
      "source": [
        "#### COLAB Utility\n",
        "Add utility function for avoiding the colab disconnection during the YOLO training of the network. This function click every 60000 milliseconds to the connect button, in this way we can avoid to be disconnected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "llI6b6yiri5H",
        "outputId": "608fd38e-f3da-409a-8f4b-13532e6ed897"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Ensure colab doesn't disconnect\n",
        "%%javascript\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}setInterval(ClickConnect,60000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfGyKL0iNQ_7"
      },
      "source": [
        "### Dependencies\n",
        "Before starting with this notebook, It is necessary to import all the different dependencies that will be used for training the network and use the trained network for tracking the different traffic signs and traffic lights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lxp6u7jrUhUc"
      },
      "outputs": [],
      "source": [
        "# Import Essential Libraries\n",
        "import os\n",
        "import random\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import pathlib\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njmmbbpNoz3M"
      },
      "source": [
        "## Strategy\n",
        "The problem encountered for this project is that, there was not a good dataset with both informations about traffic lights and also traffic signs. If we want to use DNN we need datasets with a lot of images, in order to have an high accuracy. So our idea is to use two different YOLO models, one will be trained only on traffic signs and another one on traffic lights, in this ways It will be possible to use two different specialized models for two different tasks, and by combining them It would be possible to accomplish our final task. Both dataset are formatted using the YOLO format, in this way training the system will much smoother. The ideal goal will be to have a single model capable of doing both tasks, but in order to do that we need a dataset resulting for example from the intersection of both datasets, in this way It is possible to have one single model capable of doing both tasks. But if we do so the training time will increase.\n",
        "\n",
        "### Traffic Signs Dataset\n",
        "The traffic signs dataset, that can be accessed at this link: https://universe.roboflow.com/usmanchaudhry622-gmail-com/traffic-and-road-signs, has 29 different classes and It is a subset of the GSTBR. Our first idea was to use the entire GSTBR dataset, but because of Its dimension and because we do not have the resources to train a model on such dataset, we decided to switch and use a subset of It. The original GSTBR dataset has more than 50 000 images. This dataset has exactly 10 000 different images of traffic signs, but also by having this kind of dimension It is really difficult to train the model in a short time period.\n",
        "\n",
        "#### Traffic Signs Classes\n",
        "This dataset contains 29 different classes, which are:\n",
        "- Road narrows on right\n",
        "- 50 KMPh speed limit\n",
        "- Attention Please\n",
        "- Beware of children\n",
        "- CYCLE ROUTE AHEAD WARNING\n",
        "- Dangerous Left Curve Ahead\n",
        "- Dangerous Right Curve Ahead\n",
        "- End of all speed and passing limits\n",
        "- Give Way\n",
        "- Go Straight or Turn Right\n",
        "- Go straight or turn left\n",
        "- Keep-Left\n",
        "- Keep-Right\n",
        "- Left Zig Zag Traffic\n",
        "- No Entry\n",
        "- No Over Taking\n",
        "- Overtaking by trucks is prohibited\n",
        "- Pedestrian Crossing\n",
        "- Round-About\n",
        "- Slippery Road Ahead\n",
        "- Speed Limit 20 KMPh\n",
        "- Speed Limit 30 KMPh\n",
        "- Stop Sign\n",
        "- Straight Ahead Only\n",
        "- Traffic signal\n",
        "- Truck traffic is prohibited\n",
        "- Turn left ahead\n",
        "- Turn right ahead\n",
        "- Uneven Road\n",
        "\n",
        "---\n",
        "### Traffic Lights Dataset\n",
        "The second dataset that we used is the Lisa traffic lights dataset, It has more than 9000 images of traffic lights. Even this dataset is a subset of the origial Lisa traffic Light dataset (https://www.kaggle.com/datasets/mbornoe/lisa-traffic-light-dataset) because the original one has more than 100K images. The dataset can be accessed using this link: https://universe.roboflow.com/ithb-5ka4m/lisa-traffic-light-detection-8vuch.\n",
        "\n",
        "\n",
        "#### Traffic Lights Classes\n",
        "1. go\n",
        "2. stop\n",
        "3. warning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "uaZlyznqf1J2"
      },
      "outputs": [],
      "source": [
        "def show_train_images(directory):\n",
        "    num_samples = 9\n",
        "    images_path = os.path.join(directory, \"images\")\n",
        "    labels_path = os.path.join(directory, \"labels\")\n",
        "\n",
        "    image_files = os.listdir(images_path)\n",
        "    labels_files = {os.path.splitext(file)[0]: file for file in os.listdir(labels_path)}\n",
        "\n",
        "    # Randomly select num_samples images\n",
        "    rand_images = random.sample(image_files, num_samples)\n",
        "\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(11, 11))\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        image = rand_images[i]\n",
        "        image_name = os.path.splitext(image)[0]\n",
        "        image_path = os.path.join(images_path, image)\n",
        "        label_file = labels_files.get(image_name, None)\n",
        "\n",
        "        # Load image\n",
        "        img = plt.imread(image_path)\n",
        "\n",
        "        ax = axes[i // 3, i % 3]\n",
        "        ax.imshow(img)\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Overlay labels if they exist\n",
        "        if label_file:\n",
        "            label_path = os.path.join(labels_path, label_file)\n",
        "            with open(label_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    class_id = int(parts[0])\n",
        "                    x_center, y_center, width, height = map(float, parts[1:])\n",
        "\n",
        "                    # Convert YOLO format to rectangle coordinates\n",
        "                    img_h, img_w = img.shape[:2]\n",
        "                    x1 = int((x_center - width / 2) * img_w)\n",
        "                    y1 = int((y_center - height / 2) * img_h)\n",
        "                    x2 = int((x_center + width / 2) * img_w)\n",
        "                    y2 = int((y_center + height / 2) * img_h)\n",
        "\n",
        "                    # Draw rectangle and label\n",
        "                    rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='red', facecolor='none')\n",
        "                    ax.add_patch(rect)\n",
        "                    ax.text(x1, y1 - 10, f\"Class: {class_id}\", color='red', fontsize=8, backgroundcolor='white')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Images\n",
        "Here below will be listed some of the images that are taken from both of the two datasets. As we can see the two different datasets has completely different classes, by doing this our models will be specialized for two different tasks. The first dataset contains only road sign images, instead the second dataset only contains traffic lights images."
      ],
      "metadata": {
        "id": "Yp_mrw8F2dVy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6_X7H_BUqxr"
      },
      "outputs": [],
      "source": [
        "traffic_signs_images_dir = os.path.join(traffic_signs.location, \"train\")\n",
        "traffic_lights_images_dir = os.path.join(traffic_lights.location, \"train\")\n",
        "show_train_images(traffic_signs_images_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m23WXOAgHWq"
      },
      "outputs": [],
      "source": [
        "show_train_images(traffic_lights_images_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LfXM2vaOCWx"
      },
      "source": [
        "### YOLOv11 import and training\n",
        "We trained the YOLOv11 model instand of YOLOv8 model because we needed a small model capable of performing this task with higher accuracy. Both of models will be trained for 30 epochs by using the default optimizer: Adam and using an automatic batch.\n",
        "\n",
        "#### Traffic Signs Model\n",
        "The first model that will be trained is the traffic sign model. This model will use the traffic signs dataset. Because also this dataset is very huge in dimension, the training will require a lot of time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YZd06wBU8s8"
      },
      "outputs": [],
      "source": [
        "traffic_sings_model = YOLO('yolo11n.pt')\n",
        "output_data_path = os.path.join(traffic_signs.location, \"data.yaml\")\n",
        "results = traffic_sings_model.train(data=output_data_path, epochs = 30, batch = -1, optimizer = 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Traffic Light Model\n",
        "The second model that will be trained is the traffic light model. This model will use the Lisa dataset. Here we use the same strategy as before so we train for 30 epochs using the default optimizer and using the default batch size."
      ],
      "metadata": {
        "id": "WY_L0G0J3bHA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUBCJoHXoQqg"
      },
      "outputs": [],
      "source": [
        "traffic_lights_model = YOLO('yolo11n.pt')\n",
        "output_data_path = os.path.join(traffic_lights.location, \"data.yaml\")\n",
        "results = traffic_lights_model.train(data=output_data_path, epochs = 30, batch = -1, optimizer = 'auto')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Combination of the Models\n",
        "\n",
        "Now that both models are trained, we need to combine them to process a single frame. This allows us to call both models for predictions on a given frame and return any detections to the caller.\n",
        "\n",
        "---\n",
        "#### Detectors Class\n",
        "To achieve this, we created a class called `Detectors`. This class takes a list of models as input, in this example we pass as input the traffic sign model and the traffic light model. The class exposes a `detect` method that takes a single frame as input. Internally, it calls the `single_detect` method, which uses the provided model to identify objects belonging to Its trained classes.\n",
        "\n",
        "The combined detections are returned as a list of objects. Each object includes the bounding box information (x1, y1, x2, y2) and the class name.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "l78WPXGN3vBa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example Usage\n",
        "\n",
        "```python\n",
        "\n",
        "detector = Detectors([signs_model, lights_model])\n",
        "detections = detector.detect(frame)\n",
        "\n",
        "print(detections): detections = [[100, 50, 200, 150, 'Stop Sign'], [300, 100, 400, 200, 'stop']]"
      ],
      "metadata": {
        "id": "dSWZ5zTB_FD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "TVXJdsf6pVvV"
      },
      "outputs": [],
      "source": [
        "class Detectors:\n",
        "    \"\"\"\n",
        "    A class that combines multiple detection models (e.g., traffic signs and lights)\n",
        "    to process a single frame and provide a consolidated list of detections.\n",
        "\n",
        "    Attributes:\n",
        "        models: All the models that will be used for executing the detection on the input frames.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, models: list):\n",
        "        \"\"\"\n",
        "        Initializes the Detectors class with the specified models.\n",
        "\n",
        "        Args:\n",
        "            models: List of all the models used for the detection task.\n",
        "        \"\"\"\n",
        "        self.models = list(models)\n",
        "\n",
        "    def __single_detect(self, model, frame):\n",
        "        \"\"\"\n",
        "        Uses the specified model to detect objects in the provided frame.\n",
        "\n",
        "        Args:\n",
        "            model: The detection model to use for predictions.\n",
        "            frame: The image frame to analyze.\n",
        "\n",
        "        Returns:\n",
        "            A list of detected objects. Each object is represented as a list\n",
        "            containing the bounding box coordinates (x1, y1, x2, y2) and\n",
        "            the class name.\n",
        "        \"\"\"\n",
        "        bbox_list = []\n",
        "        objs = model.predict(frame, verbose=False, conf=0.5)[0].boxes.data.cpu().numpy()\n",
        "        for det in objs:\n",
        "            x1, y1, x2, y2, conf, cls = det\n",
        "            bbox_list.append([x1, y1, x2, y2, model.names[cls]])\n",
        "        return bbox_list\n",
        "\n",
        "    def detect(self, frame):\n",
        "        \"\"\"\n",
        "        Detects objects in the given frame using all the models stored inside this class.\n",
        "\n",
        "        Args:\n",
        "            frame: The image frame to analyze.\n",
        "\n",
        "        Returns:\n",
        "            A combined list of detections very models. Each detection\n",
        "            includes bounding box coordinates and the class name.\n",
        "        \"\"\"\n",
        "        detections = []\n",
        "        for model in self.models:\n",
        "          detections.extend(self.__single_detect(model, frame))\n",
        "        return detections\n",
        "\n",
        "detector = Detectors([traffic_sings_model, traffic_lights_model])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77dmds-HOcQK"
      },
      "source": [
        "### Error handling\n",
        "Set UTF-8 as encoding for possible errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EiHqo2ixeiBa"
      },
      "outputs": [],
      "source": [
        "import locale\n",
        "\n",
        "def getpreferredencoding(do_setlocale: bool = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tUmOJqKOjmp"
      },
      "source": [
        "### Object Traking Phase\n",
        "#### Intersection over Union\n",
        "Intersection over Union (IoU) is a metric used to evaluate the accuracy of object detection and tracking systems. It measures the overlap between two bounding boxes:\n",
        "\n",
        "1. The predicted bounding box (what the model detects).\n",
        "2. The ground truth bounding box (the actual or correct bounding box).\n",
        "\n",
        "Mathematically, it is defined as:\n",
        "$$ IoU = \\frac{\\text{Area of Overlap}}{\\text{Area of Union}} $$\n",
        "Where:\n",
        "\n",
        "1. **Area of Overlap** is the region where the predicted and ground truth boxes intersect.\n",
        "2. **Area of Union** is the total area covered by both bounding boxes combined, subtracting any overlap to avoid double-counting.\n",
        "\n",
        "IoU serves as a quantitative measure of how well the predicted bounding box aligns with the ground truth. It ranges between 0 and 1:\n",
        "\n",
        "1. IoU = 0 means no overlap between the boxes.\n",
        "2. IoU = 1 means perfect overlap.\n",
        "\n",
        "When tracking an object across frames, IoU helps in determining whether the object detected in the current frame corresponds to the same object detected in the previous frame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9GLY9QHmkBPm"
      },
      "outputs": [],
      "source": [
        "def compute_iou(bb1: list, bb2: list):\n",
        "  bb1_x1=bb1[0]\n",
        "  bb1_y1=bb1[1]\n",
        "  bb1_x2=bb1[2]\n",
        "  bb1_y2=bb1[3]\n",
        "\n",
        "  bb2_x1=bb2[0]\n",
        "  bb2_y1=bb2[1]\n",
        "  bb2_x2=bb2[2]\n",
        "  bb2_y2=bb2[3]\n",
        "\n",
        "  x_left = max(bb1_x1, bb2_x1)\n",
        "  y_top = max(bb1_y1, bb2_y1)\n",
        "  x_right = min(bb1_x2, bb2_x2)\n",
        "  y_bottom = min(bb1_y2, bb2_y2)\n",
        "\n",
        "  if x_right < x_left or y_bottom < y_top:\n",
        "    return 0.0\n",
        "\n",
        "  intersection_area = (x_right - x_left) * (y_bottom - y_top)\n",
        "\n",
        "  bb1_area = (bb1_x2 - bb1_x1) * (bb1_y2 - bb1_y1)\n",
        "  bb2_area = (bb2_x2 - bb2_x1) * (bb2_y2 - bb2_y1)\n",
        "\n",
        "  iou = intersection_area / float(bb1_area + bb2_area - intersection_area)\n",
        "  assert iou >= 0.0\n",
        "  assert iou <= 1.0\n",
        "  return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaqKmUeBOwpy"
      },
      "source": [
        "### Bounding Box tracker\n",
        "The IOUTracker class is designed for simple object tracking in video or image sequences using Intersection-over-Union (IoU) as the basis for associating detected objects between frames. This tracker maintains a set of tracked objects and updates their positions based on new detections in each frame.\n",
        "\n",
        "---\n",
        "\n",
        "#### Initialization:\n",
        "1. **\\_\\_init\\_\\_**(`iou_threshold`=0.5): Initializes the tracker with the following attributes:\n",
        "iou_threshold: A float specifying the minimum IoU required to consider two bounding boxes as the same object (default is 0.5).\n",
        "2. tracked_objects: A dictionary mapping object IDs to their bounding boxes.\n",
        "next_object_id: An integer counter for assigning unique IDs to new objects.\n",
        "\n",
        "#### Methods\n",
        "\n",
        "1. `update(detections)`: Updates the set of tracked objects based on new detections.\n",
        "  * **Parameters**: `detections`: A list of bounding boxes in the format [[x1, y1, x2, y2, class name], ...], where (x1, y1) is the top-left corner, and (x2, y2) is the bottom-right corner of the bounding box.\n",
        "\n",
        "#### Process:\n",
        "For each existing tracked object, it calculates the IoU with each detection.\n",
        "Matches the detection with the highest IoU above the iou_threshold to the tracked object. Any unmatched detections are considered new objects and are assigned unique IDs. Returns:\n",
        "* An updated dictionary of tracked objects (object_id -> bounding_box).\n",
        "\n",
        "#### Key Features\n",
        "\n",
        "* Tracks objects over multiple frames by associating new detections to existing tracked objects using IoU.\n",
        "* Dynamically assigns new IDs to previously unseen objects.\n",
        "* Provides a straightforward and efficient tracking mechanism, suitable for scenarios with moderate object movement and consistent detections.\n",
        "\n",
        "---\n",
        "\n",
        "### Example Usage\n",
        "\n",
        "```python\n",
        "tracker = IOUTracker(iou_threshold=0.5)\n",
        "detections = [[100, 50, 200, 150, 'Stop Sign'], [300, 100, 400, 200, 'stop']]\n",
        "tracked_objects = tracker.update(detections)\n",
        "print(tracked_objects)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "CrHpAOT5rDpE"
      },
      "outputs": [],
      "source": [
        "class IOUTracker:\n",
        "    \"\"\"\n",
        "    A tracker that uses Intersection Over Union (IOU) to match and track objects across frames.\n",
        "\n",
        "    Attributes:\n",
        "        iou_threshold (float): The minimum IOU required to consider a detection as the same object.\n",
        "        tracked_objects (dict): A dictionary mapping object IDs to their bounding boxes.\n",
        "        next_object_id (int): The next available ID for newly detected objects.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, iou_threshold=0.5):\n",
        "        \"\"\"\n",
        "        Initializes the IOUTracker with a specified IOU threshold.\n",
        "\n",
        "        Args:\n",
        "            iou_threshold (float): The IOU threshold for associating detections with tracked objects.\n",
        "        \"\"\"\n",
        "        self.iou_threshold = iou_threshold\n",
        "        self.tracked_objects = {}  # Object ID -> Bounding Box\n",
        "        self.next_object_id = 0\n",
        "\n",
        "    def update(self, detections: list):\n",
        "        \"\"\"\n",
        "        Updates the tracker with new detections and returns the currently tracked objects.\n",
        "\n",
        "        Args:\n",
        "            detections (list): A list of bounding boxes from the current frame.\n",
        "                               Each bounding box is a list [x1, y1, x2, y2, class_name].\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary of tracked objects where keys are object IDs and values are bounding boxes.\n",
        "                  Each bounding box is in the format [x1, y1, x2, y2, class_name].\n",
        "        \"\"\"\n",
        "        updated_tracked_objects = {}\n",
        "        assigned_detections = set()\n",
        "\n",
        "        # Match existing tracked objects to new detections\n",
        "        for obj_id, prev_bbox in self.tracked_objects.items():\n",
        "            best_iou = 0\n",
        "            best_det_idx = -1\n",
        "\n",
        "            for idx, det_bbox in enumerate(detections):\n",
        "                if idx in assigned_detections:\n",
        "                    continue\n",
        "                iou = compute_iou(prev_bbox[:4], det_bbox[:4])\n",
        "                if iou > best_iou and iou >= self.iou_threshold:\n",
        "                    best_iou = iou\n",
        "                    best_det_idx = idx\n",
        "\n",
        "            if best_det_idx != -1:\n",
        "                updated_tracked_objects[obj_id] = detections[best_det_idx]\n",
        "                assigned_detections.add(best_det_idx)\n",
        "\n",
        "        # Add new detections as new objects\n",
        "        for idx, det_bbox in enumerate(detections):\n",
        "            if idx not in assigned_detections:\n",
        "                updated_tracked_objects[self.next_object_id] = det_bbox\n",
        "                self.next_object_id += 1\n",
        "\n",
        "        # Update the tracked objects\n",
        "        self.tracked_objects = updated_tracked_objects\n",
        "        return self.tracked_objects\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duubmiETOs8y"
      },
      "source": [
        "#### Video Processing with Object Detection and Tracking\n",
        "\n",
        "This function, `create_new_video`, processes a video to detect and track objects in each frame using YOLO-based detection and an IOU tracker. Here's how it works:\n",
        "\n",
        "1. **Input and Output Initialization**:\n",
        "   - The input video file is read using OpenCV, and its properties, such as frame rate, width, and height, are extracted.\n",
        "   - The output video path is generated based on the input video name, and a `VideoWriter` is set up to save the processed frames.\n",
        "\n",
        "2. **Object Detection**:\n",
        "   - Each frame is read from the video, and YOLO-based object detection is performed using the `detector` class. The detected objects, including their bounding boxes and class names, are extracted.\n",
        "\n",
        "3. **Object Tracking**:\n",
        "   - The bounding boxes are passed to an IOU-based tracker that tracks objects across frames, maintaining consistent IDs for tracked objects.\n",
        "\n",
        "4. **Frame Annotation**:\n",
        "   - Tracked objects are drawn onto each frame. This includes:\n",
        "     - Bounding boxes around detected objects.\n",
        "     - Class names annotated near the bounding boxes.\n",
        "\n",
        "5. **Output Video Generation**:\n",
        "   - Each annotated frame is written to the output video file.\n",
        "   - Once processing is complete, all resources are released, and the path to the output video is returned.\n",
        "\n",
        "This function is useful for creating videos that visually display the results of object detection and tracking models, enabling easier analysis and visualization of model performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "SuTOOPmlHNcw"
      },
      "outputs": [],
      "source": [
        "def create_new_video(video_name: str):\n",
        "    \"\"\"\n",
        "    Processes a video to detect and track objects, saving the results as a new video.\n",
        "\n",
        "    Args:\n",
        "        video_name (str): The name of the input video file.\n",
        "\n",
        "    Returns:\n",
        "        str: The file path of the processed output video.\n",
        "    \"\"\"\n",
        "    # Construct input and output video paths\n",
        "    input_path = os.path.join(traffic_lights_video_path, video_name)\n",
        "    output_video_path = video_name.split('.')[0] + \"-output.mp4\"\n",
        "\n",
        "    # Open the input video file\n",
        "    cap = cv2.VideoCapture(input_path)\n",
        "\n",
        "    # Get video properties\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Set up the video writer for the output video\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "    # Initialize the tracker\n",
        "    tracker = IOUTracker(iou_threshold=0.5)\n",
        "\n",
        "    # Process each frame of the video\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        # Perform detection on the current frame\n",
        "        results = detector.detect(frame)\n",
        "\n",
        "        # Extract bounding boxes from the detections\n",
        "        bbox_list = []\n",
        "        for det in results:\n",
        "            x1, y1, x2, y2, cls = det\n",
        "            bbox_list.append([x1, y1, x2, y2, cls])\n",
        "\n",
        "        # Update tracker with current frame's bounding boxes\n",
        "        tracked_objects = tracker.update(bbox_list)\n",
        "\n",
        "        # Draw tracked objects on the frame\n",
        "        for obj_id, bbox in tracked_objects.items():\n",
        "            x1, y1, x2, y2 = map(int, bbox[:4])\n",
        "            class_name = bbox[4]\n",
        "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "            cv2.putText(frame, f'Class: {class_name}', (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
        "\n",
        "        # Write the processed frame to the output video\n",
        "        out.write(frame)\n",
        "\n",
        "    # Release resources\n",
        "    cap.release()\n",
        "    out.release()\n",
        "\n",
        "    return output_video_path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "surtlUqHVtLx"
      },
      "source": [
        "### Test a video\n",
        "Now that It was possible to train our system we can test the YOLO architecture on a video, by using the create\\_new\\_video function, then after parsing the video and checking If in each frame there is an possible object we create the output video that is possible to visualize by the following cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fncUAgJXZ1eV"
      },
      "outputs": [],
      "source": [
        "  print(os.listdir(traffic_lights_video_path))\n",
        "  video = create_new_video(\"traffic_lights_red_1.mp4\")\n",
        "  mp4 = open(video,'rb').read()\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "\n",
        "  HTML(\"\"\" <video controls><source src=\"%s\" type=\"video/mp4\"></video>\"\"\" % data_url)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}